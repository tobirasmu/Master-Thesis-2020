\section{Discussion} \label{tex:discussion}

In this section the results from the previous section will be discussed in order to cast light on discrepancies, and to give potential explanations for them. This section will also be partitioned due to the two methods that have been examined in this report. Subsequently, there will be a discussion for when to use which method, i.e. what kind of problems are relevant for each of the methods. In the end \autoref{tex:future_work} will discuss possible directions to take the research even further.

The theoretical speed-up have been calculated using the number of FLOPs it takes to perform a forward push through the given model. In calculating the FLOPs for the different types of layers, it was assumed that the non-linear transforms, reshaping, and pooling layers were for free, hence that calculation time only has contributions from the convolutional and linear layers themselves. Obviously everything costs time, but this assumption is deemed fair to make because the time it takes to do these things is so low relative to the computations carried out in the layers. This does however mean that the times reported in the previous section are polluted with transformations, poolings, and reshapings, and that for simpler architectures with less total FLOPs it has a bigger impact on the timing result. Even though the timing results are polluted, they are still comparable to each other because both the original architecture and the compressed version suffer the same additional operations, i.e. the same time pollution.


\subsection{Method 1}
From the results of the experiment with Method 1 in \autoref{tex:result_experiment}, it is clear that the decomposition is good at finding the dissimilarities between different observations. Using a low rank of only 2 and a simple ANN, it was actually possible to achieve a high accuracy for MNIST 3s and 4s while the THETIS forehands and backhands were not as easily distinguished. This is due to the simplicity of the MNIST data set and that the 3s and 4s look very different from each other in general. This stands in contrast to the THETIS data set which is far more complex and where the key to a proper classification lies in the temporal details. In the THETIS case the algorithm did not separate the types of shot even though it was given full temporal information, but rather the setting at which the videos were taken, which in hindsight makes sense since the background is constant through every video. When trying to simplify something that complex the algorithm is looking for the biggest dissimilarity which in this case is the background.

Increasing the rank allows the algorithm to look for finer details. In the case of the THETIS data set a rank of only 30 increases the testing accuracy quite remarkably, which means that already at this rank it is able to see enough details to make a qualified decision. Increasing the rank even further implies an increase in the testing accuracy, but unfortunately the rank needs to exceed 120 in order to meet the accuracy of simply using the full input into the ANN. This is unfortunate because the computational complexity and the time is exceeded already at a rank of $\approx 100$. In the case of the MNIST data set the accuracy of simply using the full input is never reached by the compressed version (at least not for the ranks tested), even though more than 7 times the number of FLOPs and parameters are used. The time using the CPU exceeds that of the full input model already at a rank of $\approx 12$, while on the GPU it is never faster to run the compressed model. This is assumed to be due to the low number of FLOPs, and the speed of the GPU. The lack of accuracy is assumed due to the increase in complexity resulting after adding more digits, and that some digits (e.g. 4 and 9) look very similar and gives the decomposition algorithm a hard time to distinguish them.

In general it seems that for this method the number of FLOPs and the actual computation time are correlated, but it also seems like there is some kind of time penalty for doing the approximation of the input loadings in contrast to just using them as input into the ANN. For instance the GPU times for the MNIST data set in \autoref{tab:res_input_MNIST} that are almost constant even though the number of FLOPs vary quite remarkably. In this case the time of the approximation of the loadings seem to increase with the rank, while the push through the simple ANN is constant throughout, implying that there is a penalty for just using a NN.

All in all this method does not seem appropriate due to the need of the rank to be high in order to get comparable accuracies. Especially the need for the rank to exceed the point at which it is advantageous to use the compressed input, makes this method questionable. It seems that the method can distinguish very simple problems that look very different from each other well, but as soon as the complexity rises and observations look relatively alike it has a hard time. The method would however be appropriate if the observations in the same class look alike and between classes do not, as for instance if the goal for the THETIS data set was to classify the location of the video ignoring the person moving in front.

\subsection{Method 2}
While the architecture for the MNIST data set is chosen based on previous results and because it is known to achieve good results, the architecture for the THETIS data set is chosen purely based on intuition. Simply expanding the approximate architecture for MNIST seem appropriate because the whole video sequence is a part of the shot. The videos does not contain constant general movement, but rather a single movement. This makes a single frame unimportant, but a sequence of frames crucial to learning the data set. Simply making a CNN with a 3D or video kernel is called \textit{slow fusion}, because the temporal information is passed on slowly. Other methods for video classification could potentially work. These include \textit{early fusion}\footnote{In early fusion a 2D convolution is used on each frame and the results are then immediately stacked in order to look for temporal movement} and \textit{late fusion}\footnote{In late fusion the individual frames are first learned using a 2D CNN, and the temporal information is only considered in the end}\cite{Karpathy2014}.

Only compressing the input and output dimensions in the convolution seems smart because the kernel is important (what the computer "sees") and also it is often small in dimensions. The different filters (output channels) might look for the same thing why exploiting the cross-filter redundancy is useful. Also the input channels (an RGB image) are often redundant since they show the same thing, i.e. they have the same lines and structures of the image. This all makes it appropriate to compress the input / output channels and leaving the rest uncompressed.

From the results in the last section it is clear that the time cannot be explained simply by the number of FLOPs, but that more is going on in the back-end. In no case was the compressed model sped-up as much as was predicted using the number of FLOPs, and using the MNIST architecture the compressed model was actually slower than the original both using the CPU and the GPU. Kim et al. achieved something similar\cite{Kim2016}, since nor they are able to match the theoretically expected speed-up. For the VGG-16 architecture their speed-up of 2.33 times is comparable to the 2.03 times speed-up achieved in this thesis running on the CPU. It should be noted that they only compress the convolutional layers and not the linear layers as done in this work. This goes to show that the storage improvement achieved in this thesis is much greater than what is achieved in their work ($\times 6.07$ vs. $\times 1.09$). They make two interesting observations that they use to explain the lack of observed speed-up, that might also be applicable to the results achieved in this thesis. First of all they give blame to the $1\times 1 (\times 1)$ convolution for not being cache efficient\footnote{The cache is a hardware component that saves data used in computations in order to very quickly access and use them again. Proper use of the cache memory results in very fast computations}, which means that even though it should use less FLOPs, each FLOP takes more time to carry out. Secondly they observe that using the computing power on an actual smartphone, they manage to get a bigger speed-up, since the smartphone does not match a computer in number of threads or in the power to parallelize. These results make it seem like the more computer-power you have, the smaller the observed speed-up will be. 

The proposed $1\times 1 (\times 1)$ convolution cache inefficiency is also to be found in the results given in  the previous section. For instance for the Conv 2 layer from the compressed MNIST architecture, the first and last layers are both $1\times 1$ convolutions and take up the majority of the time using 33\% and 45 \% of the time even though they only correspond to 20\% and 10\% of the FLOPs respectively (see \autoref{tab:res_MNIST_FLOPs} and \autoref{tab:res_MNIST_time}). The inefficiency in this case is so pronounced that it makes the whole network slower than the corresponding original network. For Conv 2 in the THETIS architecture the same pattern is shown, though not as pronounced and not enough to make it slower than its corresponding original. Here the $1\times 1 \times 1$ convolutions require only 0.9\% and 1.1\% of the FLOPs respectively, but ends up taking 24\% and 16\% of the time.

The lack of speed in the linear layers seems harder to address because the linear layers simply consist of a matrix-vector product that is a very well-known operation, and because the linear layers are not as prone to time-pollution in terms of reshaping or pooling. Looking at the GPU times for the MNIST architecture the time seems relatively constant through the layers even though they require quiet different amounts of FLOPs. This makes it seem like there is some penalty for starting up a new layer, which means that adding more layers causes a bigger time penalty. This is however only a hypothesis and does not have any scientific backing.

The hypothesis that more computing power, i.e. more threads and more power to parallelize, causes a smaller observed speed-up is not backed by the results in this thesis. Both for the MNIST, THETIS, and VGG-16 architectures the speed-up on the GPU is the same or greater than that of the CPU. Even though these are different means of computation, they are however still both used on a computer with greater computing power than a smartphone. It would require the means of running the algorithms on a smartphone to investigate this hypothesis further. Nevertheless the power of the GPU comes from its ability to perform many computations in a short period of time, e.g. by parallelizing. This means that sometimes the computations are carried out in a weird way that makes it hard to time the individual layers properly. This is especially expressed in the results for the THETIS architecture where the Conv 2 layer ends up taking much more time than the original layer, and the Lin 2 layer is performed much faster. Also the last convolution in the sequence of the compressed Conv 2 layer takes much more time than it should.

The rank selection for Method 2 is carried out by the use of VBMF. The goal of VBMF is not to find the optimal ranks for the whole problem, but to find the lowest rank that makes the decomposition give a good approximation of the original kernel. These ranks have shown to be a good heuristic for the rank selection for Method 2. This method does however not have the ability to decompose the linear layers with respect to the input channel dimension. The choice in this thesis have been to simply decompose with respect to the output channel, and for Tucker-2 decomposition of the linear layer, use the output-channel dimension rank for both. This acts again as a heuristic, but might give too small ranks since the input dimension is often greater than the output dimension. This has however not caused any problems for the results achieved in this thesis, but is a good stepping stone for further experiments.

All in all it seems Method 2 does have the power to bring down the number of weights and FLOPs quite remarkably, while maintaining the same accuracy as the full model. The expected speed-up is however never met by the compressed layer, and for simple architectures such as the MNIST architecture given in \autoref{fig:architecture_choices} it simply does not make sense to carry out the decomposition if a faster evaluation is the goal - at least from the results achieved here. Greater speed-ups could potentially be achieved if running and timing the evaluations on a smartphone or another device with a low computing power relative to a computer.

\subsection{When to Use Which Method}
In this section a small discussion of when and for which type of problem each method is relevant based on the results achieved in this thesis.

Method 1 allows for decreasing the input dimensions into a NN and seems like a good choice if some structure is already given, i.e. observations in the same class are similar and observations from different classes have clear dissimilarities. These requirements are not met by many classification problems, why the potential applications of this method seem limited. This is also due to the fact that many simpler ML models might do a better job at classifying a data set this simple.

Method 2 has many advantages because it is based on a model that have already been trained and that is known to work properly. Method 2 is in some way like pruning the network by removing some parameters, but parameters are removed in a smart way instead of just removing them as in usual pruning. This method allows for training a relatively complex model on a powerful computer, and then compressing it in order to make evaluating it less time and space consuming on devices with less computing power. One could potentially arrive at a relevant solution faster, because of the ability to try an over-parametrized model instead of searching for the simplest model to solve the problem in the first place. This all implies that Method 2 is appropriate for any task that requires an ANN or a CNN for either pictures or videos. What also makes Method 2 smart is that it is relatively easy to compress the entire model in one shot using approximately the same method for the different layers. This makes implementation relatively straight forward. The compression itself have scientific backing that is quite easy to understand. 

\subsection{Future Work} \label{tex:future_work}
There are many ways to take this work even further; both other ways of investigating the same methods and improving the results, but also applying other methods to the same problem. Some potential ways of taking the research further is given in the following. Method 1 is however not discussed due to its limited potential.

For Method 2, the natural thing to do next would be to try running the networks on a smartphone or a device with a limited amount of computing power, since this would potentially give better speed-up results. Doing this for different problems in terms of size gives the ability to examine the impact of the compression method on networks of various sizes.

Another potential research topic would be to evaluate the performance of the VBMF method for rank selection. Using smaller ranks would also increase the speed-up, and could potentially give a small or no accuracy drop. The weight approximation does not need to be perfect due to the subsequent fine-tuning as is also discussed by Lebedev et al. in \cite{Lebedev2015}. Additionally having a small accuracy drop might not be bad for some problems if the algorithm is able to run much faster. In relation to the rank selection, other methods could also be investigated. Especially there is a need for finding a way to standardize the rank selection making it able to calculate the rank for the input dimension of the linear layer. To start off one could investigate simpler heuristics such as a fixed fraction of the number of neurons / nodes.

The $1\times 1 (\times 1)$ convolution has shown to be cache-inefficient. Investigation of the causes of this inefficiency along with how to perform this type of convolution more efficiently would be valuable. An idea could be to reshape or translate the input such that the same operation would be carried out from another angle, making them more accessible in the cache.

Other methods for doing the decomposition itself could also be tested. As mentioned in \autoref{tex:previous_work} many different methods have shown promising results. For example it would be interesting to combine TT-decomposition of the linear layers with BTD of the convolutional layers, since these achieved good results for their respective layer. One could also try a different combination of methods, though combining methods would mean that more methods need to be understood potentially raising the complexity and the difficulty of implementation. One could also compress the network in a totally different way, e.g. not compressing post training, but as a part of the training. The \texttt{autograd} package in \texttt{PyTorch} is very powerful and could possibly be used to train variables being compressed as a part of the network. This means that variables would both be compressed and updated / trained at every epoch of the training process.