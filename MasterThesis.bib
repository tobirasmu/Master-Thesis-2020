Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{MNIST,
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.},
title = {{The MNIST database of handwritten digits}},
year = {1998}
}
@article{Olikier2016,
author = {Olikier, Guillaume},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/blockTermDecomposition.pdf:pdf},
journal = {{\'{E}}cole Polytech. Louvain},
title = {{Tensor approximation by block term decomposition}},
year = {2016}
}
@article{Lebedev2015,
abstract = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1{\%} from 91{\%} to 90{\%}). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1{\%} increase of the overall top-5 classification error.},
archivePrefix = {arXiv},
arxivId = {1412.6553},
author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
eprint = {1412.6553},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Speeding-up CNNs using CP.pdf:pdf},
journal = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
pages = {1--11},
title = {{Speeding-up convolutional neural networks using fine-tuned CP-decomposition}},
year = {2015}
}
@article{Beavers2013,
abstract = {In this 2013 winner of the prestigious R.R. Hawkins Award from the Association of American Publishers, as well as the 2013 PROSE Awards for Mathematics and Best in Physical Sciences {\&} Mathematics, also from the AAP, readers will find many of the most significant contributions from the four-volume set of the Collected Works of A. M. Turing. These contributions, together with commentaries from current experts in a wide spectrum of fields and backgrounds, provide insight on the significance and contemporary impact of Alan Turing's work. Offering a more modern perspective than anything currently available, Alan Turing: His Work and Impact gives wide coverage of the many ways in which Turing's scientific endeavors have impacted current research and understanding of the world. His pivotal writings on subjects including computing, artificial intelligence, cryptography, morphogenesis, and more display continued relevance and insight into today's scientific and technological landscape. This collection provides a great service to researchers, but is also an approachable entry point for readers with limited training in the science, but an urge to learn more about the details of Turing's work.2013 winner of the prestigious R.R. Hawkins Award from the Association of American Publishers, as well as the 2013 PROSE Awards for Mathematics and Best in Physical Sciences {\&} Mathematics, also from the AAPNamed a 2013 Notable Computer Book in Computing Milieux by Computing ReviewsAffordable, key collection of the most significant papers by A.M. TuringCommentary explaining the significance of each seminal paper by preeminent leaders in the fieldAdditional resources available online},
author = {Beavers, Anthony F},
file = {:Users/Tobias/Downloads/10.1.1.225.255.pdf:pdf},
isbn = {978-0-12-386980-7},
journal = {Alan Turing His Work Impact},
pages = {481--485},
title = {{Alan Turing : Mathematical Mechanist}},
year = {2013}
}
@article{MÃ¸rup2011,
abstract = {Tensor (multiway array) factorization and decomposition has become an important tool for datamining. Fueled by the computational power of modern computer researchers can now analyze large-scale tensorial structured data that only a few years ago would have been impossible. Tensor factorizations have several advantages over two-way matrix factorizations including uniqueness of the optimal solution and component identification even when most of the data is missing. Furthermore, multiway decomposition techniques explicitly exploit the multiway structure that is lost when collapsing some of the modes of the tensor in order to analyze the data by regular matrix factorization approaches. Multiway decomposition is being applied to new fields every year and there is no doubt that the future will bring many exciting new applications. The aim of this overview is to introduce the basic concepts of tensor decompositions and demonstrate some of themany benefits and challenges of modeling data multiway for a wide variety of data and problem domains. {\textcopyright} 2011 John Wiley {\&} Sons, Inc.},
author = {M{\o}rup, Morten},
doi = {10.1002/widm.1},
file = {:Users/Tobias/Downloads/widm.1.pdf:pdf},
issn = {19424787},
journal = {Wiley Interdiscip. Rev. Data Min. Knowl. Discov.},
number = {1},
pages = {24--40},
title = {{Applications of tensor (multiway array) factorizations and decompositions in data mining}},
volume = {1},
year = {2011}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/Tobias/Downloads/Yosinski{\_}{\_}2015{\_}{\_}ICML{\_}DL{\_}{\_}Understanding{\_}Neural{\_}Networks{\_}Through{\_}Deep{\_}Visualization{\_}{\_}.pdf:pdf},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Kossaifi2016,
abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed $\backslash$emph{\{}TensorLy{\}}, a high-level API for tensor methods and deep tensorized neural networks in Python. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and seamlessly integrates with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows users to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
archivePrefix = {arXiv},
arxivId = {1610.09555},
author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
eprint = {1610.09555},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/3322706.3322732.pdf:pdf},
journal = {J. Mach. Learn. Res. 20},
pages = {1--6},
title = {{TensorLy: Tensor Learning in Python}},
url = {http://arxiv.org/abs/1610.09555},
volume = {20},
year = {2016}
}
@article{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {1509.06569},
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1509.06569},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/TensorizingNeuralNetworks.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {442--450},
title = {{Tensorizing neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N â¥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. {\textcopyright} 2009 Society for Industrial and Applied Mathematics.},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/25662308.pdf:pdf},
issn = {00361445},
journal = {SIAM Rev.},
keywords = {Canonical decomposition (CANDECOMP),Higher-order principal components analysis (Tucker,Higher-order singular value decomposition (HOSVD),Multilinear algebra,Multiway arrays,Parallel factors (PARAFAC),Tensor decompositions},
number = {3},
pages = {455--500},
title = {{Tensor decompositions and applications}},
volume = {51},
year = {2009}
}
@misc{deepLearning,
author = {Wikipedia},
booktitle = {Wikipedia.org},
title = {{Deep Learning}},
urldate = {2020-09-16}
}
@article{Khuriwal2018,
abstract = {The cancer is the most dangerous diseases in the world, its mainly effective for women. So, our prime target must be curing the cancer through scientific investigation and the second main target should be early detection of cancer because the early detection of cancer can be helpful for remove the cancer completed. After reviewed 41 papers we found that several techniques are available for cancer detection. In this paperwe proposedDeep Leaning algorithm neural network for diagnosed breast cancer using Wisconsin Breast Cancer database. The paper shows how we can use deep learning technology for diagnosis breast cancer using UCI Dataset. Because deep learning techniques almost used for high task objective Computer Vision, Image processing, Medical Diagnosis, Neural Language Processing. But in this paper, we are applying deep learning technology on the Wisconsin Breast Cancer Database and we have seen that is very beneficial for us for diagnosis breast cancer with accuracy 99.67{\%}. This paper is divided in three parts first we have collect dataset and applied pre-processing algorithm for scaled and filter data then we have split dataset in training and testing purpose and generate some graph for visualization data. In last implement model on training dataset and achieved accuracy 99.67{\%}. So, we have seen deep learning technology is a good way for diagnosis breast cancer with Wisconsin Breast Dataset. This database provides 569 rows and 30 features in the dataset. In this paper we have used 11 features for diagnosis breast cancer that we have got after pre-processing. But before train model we have applied some pre-processing algorithm like Label Encoder, Normalizerand StandardScalerfor scaled dataset then applied model and achieved accuracy. In this paper we also compare deep learning algorithm with other machine learning and seen our proposed system is proved best from others machine learning algorithm.},
author = {Khuriwal, Naresh and Mishra, Nidhi},
doi = {10.1109/ICACCCN.2018.8748777},
file = {:Users/Tobias/Downloads/08748777.pdf:pdf},
isbn = {9781538641194},
journal = {Proc. - IEEE 2018 Int. Conf. Adv. Comput. Commun. Control Networking, ICACCCN 2018},
keywords = {Convolutional Neural Network,Deep Learning,MachineLearning,Neural Network,Random Forest,Support Vector Machine,WDBCDataset},
pages = {98--103},
publisher = {IEEE},
title = {{Breast Cancer Diagnosis Using Deep Learning Algorithm}},
year = {2018}
}
@article{Denil2013,
abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95{\%} of the weights of a network without any drop in accuracy.},
archivePrefix = {arXiv},
arxivId = {1306.0543},
author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'aurelio and {De Freitas}, Nando},
eprint = {1306.0543},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Predicting parameters in deep learning.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
title = {{Predicting parameters in deep learning}},
year = {2013}
}
@article{Wang2016,
abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in a wide range of computer vision tasks, typically at the cost of massive computational complexity. The low speed of these networks may hinder realtime applications especially when computational resources are limited. In this paper, an efficient and effective approach is proposed to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. Specifically, for each convolutional layer, the kernel tensor is decomposed into the sum of a small number of low multilinear rank tensors. Then we replace the original kernel tensors in all layers with the approximate tensors and fine-tune the whole net with respect to the final classification task using standard backpropagation. Comprehensive experiments on ILSVRC-12 demonstrate significant reduction in computational complexity, at the cost of negligible loss in accuracy. For the widely used VGG-16 model, our approach obtains a 6.6Ã speed-up on PC and 5.91Ã speed-up on mobile device of the whole network with less than 1{\%} increase on top-5 error.},
author = {Wang, Peisong and Cheng, Jian},
doi = {10.1145/2964284.2967280},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Accelerating CNNs for Mobile Applications.pdf:pdf},
isbn = {9781450336031},
journal = {MM 2016 - Proc. 2016 ACM Multimed. Conf.},
keywords = {Acceleration,Convolutional neural networks,Image classification,Tensor decomposition},
pages = {541--545},
title = {{Accelerating convolutional neural networks for mobile applications}},
year = {2016}
}
@article{tensorly,
author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
journal = {J. Mach. Learn. Res.},
pages = {1--6},
title = {{Tensorly: Tensor Learning in Python}},
volume = {20},
year = {2019}
}
@article{Liu2015,
abstract = {Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90{\%} of parameters, with a drop of accuracy that is less than 1{\%} on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.},
author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
doi = {10.1109/CVPR.2015.7298681},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Introduction/07298681.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {806--814},
publisher = {IEEE},
title = {{Sparse Convolutional Neural Networks}},
volume = {07-12-June},
year = {2015}
}
@misc{turing,
author = {Wikipedia},
booktitle = {Wikipedia.org},
title = {{Alan Turing - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Alan{\_}Turing},
urldate = {2020-09-16},
year = {2020}
}
