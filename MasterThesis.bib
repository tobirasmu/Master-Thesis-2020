Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Lebedev2015,
abstract = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1{\%} from 91{\%} to 90{\%}). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1{\%} increase of the overall top-5 classification error.},
archivePrefix = {arXiv},
arxivId = {1412.6553},
author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
eprint = {1412.6553},
file = {:Users/Tobias/Desktop/General/Speeding-up CNNs using CP.pdf:pdf},
journal = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
pages = {1--11},
title = {{Speeding-up convolutional neural networks using fine-tuned CP-decomposition}},
year = {2015}
}
@article{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {1509.06569},
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1509.06569},
file = {:Users/Tobias/Desktop/General/TensorizingNeuralNetworks.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {442--450},
title = {{Tensorizing neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Wang2016,
abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in a wide range of computer vision tasks, typically at the cost of massive computational complexity. The low speed of these networks may hinder realtime applications especially when computational resources are limited. In this paper, an efficient and effective approach is proposed to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. Specifically, for each convolutional layer, the kernel tensor is decomposed into the sum of a small number of low multilinear rank tensors. Then we replace the original kernel tensors in all layers with the approximate tensors and fine-tune the whole net with respect to the final classification task using standard backpropagation. Comprehensive experiments on ILSVRC-12 demonstrate significant reduction in computational complexity, at the cost of negligible loss in accuracy. For the widely used VGG-16 model, our approach obtains a 6.6× speed-up on PC and 5.91× speed-up on mobile device of the whole network with less than 1{\%} increase on top-5 error.},
author = {Wang, Peisong and Cheng, Jian},
doi = {10.1145/2964284.2967280},
file = {:Users/Tobias/Desktop/General/Accelerating CNNs for Mobile Applications.pdf:pdf},
isbn = {9781450336031},
journal = {MM 2016 - Proc. 2016 ACM Multimed. Conf.},
keywords = {Acceleration,Convolutional neural networks,Image classification,Tensor decomposition},
pages = {541--545},
title = {{Accelerating convolutional neural networks for mobile applications}},
year = {2016}
}
