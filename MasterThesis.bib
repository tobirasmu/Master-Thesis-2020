Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{tensorly,
author = {Kassaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
journal = {J. Mach. Learn. Res.},
pages = {1--6},
title = {{Tensorly: Tensor Learning in Python}},
volume = {20},
year = {2019}
}
@article{Kossaifi2016,
abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed $\backslash$emph{\{}TensorLy{\}}, a high-level API for tensor methods and deep tensorized neural networks in Python. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and seamlessly integrates with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows users to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
archivePrefix = {arXiv},
arxivId = {1610.09555},
author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
eprint = {1610.09555},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/3322706.3322732.pdf:pdf},
journal = {J. Mach. Learn. Res. 20},
pages = {1--6},
title = {{TensorLy: Tensor Learning in Python}},
url = {http://arxiv.org/abs/1610.09555},
volume = {20},
year = {2016}
}
@article{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {1509.06569},
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1509.06569},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/TensorizingNeuralNetworks.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {442--450},
title = {{Tensorizing neural networks}},
volume = {2015-Janua},
year = {2015}
}
@article{Lebedev2015,
abstract = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1{\%} from 91{\%} to 90{\%}). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1{\%} increase of the overall top-5 classification error.},
archivePrefix = {arXiv},
arxivId = {1412.6553},
author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
eprint = {1412.6553},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Speeding-up CNNs using CP.pdf:pdf},
journal = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
pages = {1--11},
title = {{Speeding-up convolutional neural networks using fine-tuned CP-decomposition}},
year = {2015}
}
@article{Olikier2016,
author = {Olikier, Guillaume},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/blockTermDecomposition.pdf:pdf},
journal = {{\'{E}}cole Polytech. Louvain},
title = {{Tensor approximation by block term decomposition}},
year = {2016}
}
@article{Wang2016,
abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in a wide range of computer vision tasks, typically at the cost of massive computational complexity. The low speed of these networks may hinder realtime applications especially when computational resources are limited. In this paper, an efficient and effective approach is proposed to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. Specifically, for each convolutional layer, the kernel tensor is decomposed into the sum of a small number of low multilinear rank tensors. Then we replace the original kernel tensors in all layers with the approximate tensors and fine-tune the whole net with respect to the final classification task using standard backpropagation. Comprehensive experiments on ILSVRC-12 demonstrate significant reduction in computational complexity, at the cost of negligible loss in accuracy. For the widely used VGG-16 model, our approach obtains a 6.6× speed-up on PC and 5.91× speed-up on mobile device of the whole network with less than 1{\%} increase on top-5 error.},
author = {Wang, Peisong and Cheng, Jian},
doi = {10.1145/2964284.2967280},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Accelerating CNNs for Mobile Applications.pdf:pdf},
isbn = {9781450336031},
journal = {MM 2016 - Proc. 2016 ACM Multimed. Conf.},
keywords = {Acceleration,Convolutional neural networks,Image classification,Tensor decomposition},
pages = {541--545},
title = {{Accelerating convolutional neural networks for mobile applications}},
year = {2016}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. {\textcopyright} 2009 Society for Industrial and Applied Mathematics.},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/25662308.pdf:pdf},
issn = {00361445},
journal = {SIAM Rev.},
keywords = {Canonical decomposition (CANDECOMP),Higher-order principal components analysis (Tucker,Higher-order singular value decomposition (HOSVD),Multilinear algebra,Multiway arrays,Parallel factors (PARAFAC),Tensor decompositions},
number = {3},
pages = {455--500},
title = {{Tensor decompositions and applications}},
volume = {51},
year = {2009}
}
@article{Mørup2011,
abstract = {Tensor (multiway array) factorization and decomposition has become an important tool for datamining. Fueled by the computational power of modern computer researchers can now analyze large-scale tensorial structured data that only a few years ago would have been impossible. Tensor factorizations have several advantages over two-way matrix factorizations including uniqueness of the optimal solution and component identification even when most of the data is missing. Furthermore, multiway decomposition techniques explicitly exploit the multiway structure that is lost when collapsing some of the modes of the tensor in order to analyze the data by regular matrix factorization approaches. Multiway decomposition is being applied to new fields every year and there is no doubt that the future will bring many exciting new applications. The aim of this overview is to introduce the basic concepts of tensor decompositions and demonstrate some of themany benefits and challenges of modeling data multiway for a wide variety of data and problem domains. {\textcopyright} 2011 John Wiley {\&} Sons, Inc.},
author = {M{\o}rup, Morten},
doi = {10.1002/widm.1},
file = {:Users/Tobias/Downloads/widm.1.pdf:pdf},
issn = {19424787},
journal = {Wiley Interdiscip. Rev. Data Min. Knowl. Discov.},
number = {1},
pages = {24--40},
title = {{Applications of tensor (multiway array) factorizations and decompositions in data mining}},
volume = {1},
year = {2011}
}
