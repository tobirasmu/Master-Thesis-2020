Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Sironi2015,
abstract = {Learning filters to produce sparse image representations in terms of overcomplete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-the-art methods on the curvilinear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic convolutional filter banks to reduce the complexity of the feature extraction step.},
author = {Sironi, Amos and Tekin, Bugra and Rigamonti, Roberto and Lepetit, Vincent and Fua, Pascal},
doi = {10.1109/TPAMI.2014.2343229},
file = {:Users/Tobias/Downloads/separable{\_}filters{\_}learning{\_}1.pdf:pdf},
issn = {01628828},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
keywords = {Convolutional neural networks,Convolutional sparse coding,Features extraction,Filter learning,Image denoising,Segmentation of linear structures,Separable convolution,Tensor decomposition},
number = {1},
pages = {94--106},
title = {{Learning separable filters}},
volume = {37},
year = {2015}
}
@article{Olikier2016,
author = {Olikier, Guillaume},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/blockTermDecomposition.pdf:pdf},
journal = {{\'{E}}cole Polytech. Louvain},
title = {{Tensor approximation by block term decomposition}},
year = {2016}
}
@article{Khuriwal2018,
abstract = {The cancer is the most dangerous diseases in the world, its mainly effective for women. So, our prime target must be curing the cancer through scientific investigation and the second main target should be early detection of cancer because the early detection of cancer can be helpful for remove the cancer completed. After reviewed 41 papers we found that several techniques are available for cancer detection. In this paperwe proposedDeep Leaning algorithm neural network for diagnosed breast cancer using Wisconsin Breast Cancer database. The paper shows how we can use deep learning technology for diagnosis breast cancer using UCI Dataset. Because deep learning techniques almost used for high task objective Computer Vision, Image processing, Medical Diagnosis, Neural Language Processing. But in this paper, we are applying deep learning technology on the Wisconsin Breast Cancer Database and we have seen that is very beneficial for us for diagnosis breast cancer with accuracy 99.67{\%}. This paper is divided in three parts first we have collect dataset and applied pre-processing algorithm for scaled and filter data then we have split dataset in training and testing purpose and generate some graph for visualization data. In last implement model on training dataset and achieved accuracy 99.67{\%}. So, we have seen deep learning technology is a good way for diagnosis breast cancer with Wisconsin Breast Dataset. This database provides 569 rows and 30 features in the dataset. In this paper we have used 11 features for diagnosis breast cancer that we have got after pre-processing. But before train model we have applied some pre-processing algorithm like Label Encoder, Normalizerand StandardScalerfor scaled dataset then applied model and achieved accuracy. In this paper we also compare deep learning algorithm with other machine learning and seen our proposed system is proved best from others machine learning algorithm.},
author = {Khuriwal, Naresh and Mishra, Nidhi},
doi = {10.1109/ICACCCN.2018.8748777},
file = {:Users/Tobias/Downloads/08748777.pdf:pdf},
isbn = {9781538641194},
journal = {Proc. - IEEE 2018 Int. Conf. Adv. Comput. Commun. Control Networking, ICACCCN 2018},
keywords = {Convolutional Neural Network,Deep Learning,MachineLearning,Neural Network,Random Forest,Support Vector Machine,WDBCDataset},
pages = {98--103},
publisher = {IEEE},
title = {{Breast Cancer Diagnosis Using Deep Learning Algorithm}},
year = {2018}
}
@article{Simonyan2015,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {arXiv:1409.1556v6},
file = {:Users/Tobias/Downloads/1409.1556.pdf:pdf},
journal = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
pages = {1--14},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2015}
}
@article{Kolda2009,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N-way array. Decompositions of higher-order tensors (i.e., N-way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, and elsewhere. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decomposition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal component analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox, Tensor Toolbox, and Multilinear Engine are examples of software packages for working with tensors. {\textcopyright} 2009 Society for Industrial and Applied Mathematics.},
author = {Kolda, Tamara G. and Bader, Brett W.},
doi = {10.1137/07070111X},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/25662308.pdf:pdf},
issn = {00361445},
journal = {SIAM Rev.},
keywords = {Canonical decomposition (CANDECOMP),Higher-order principal components analysis (Tucker,Higher-order singular value decomposition (HOSVD),Multilinear algebra,Multiway arrays,Parallel factors (PARAFAC),Tensor decompositions},
number = {3},
pages = {455--500},
title = {{Tensor decompositions and applications}},
volume = {51},
year = {2009}
}
@article{Wang2016,
abstract = {Convolutional neural networks (CNNs) have achieved remarkable performance in a wide range of computer vision tasks, typically at the cost of massive computational complexity. The low speed of these networks may hinder realtime applications especially when computational resources are limited. In this paper, an efficient and effective approach is proposed to accelerate the test-phase computation of CNNs based on low-rank and group sparse tensor decomposition. Specifically, for each convolutional layer, the kernel tensor is decomposed into the sum of a small number of low multilinear rank tensors. Then we replace the original kernel tensors in all layers with the approximate tensors and fine-tune the whole net with respect to the final classification task using standard backpropagation. Comprehensive experiments on ILSVRC-12 demonstrate significant reduction in computational complexity, at the cost of negligible loss in accuracy. For the widely used VGG-16 model, our approach obtains a 6.6× speed-up on PC and 5.91× speed-up on mobile device of the whole network with less than 1{\%} increase on top-5 error.},
author = {Wang, Peisong and Cheng, Jian},
doi = {10.1145/2964284.2967280},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Accelerating CNNs for Mobile Applications.pdf:pdf},
isbn = {9781450336031},
journal = {MM 2016 - Proc. 2016 ACM Multimed. Conf.},
keywords = {Acceleration,Convolutional neural networks,Image classification,Tensor decomposition},
pages = {541--545},
title = {{Accelerating convolutional neural networks for mobile applications}},
year = {2016}
}
@article{tensorly,
author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
file = {:Users/Tobias/Downloads/18-277.pdf:pdf},
journal = {J. Mach. Learn. Res.},
pages = {1--6},
title = {{Tensorly: Tensor Learning in Python}},
volume = {20},
year = {2019}
}
@misc{turing,
author = {Wikipedia},
booktitle = {Wikipedia.org},
title = {{Alan Turing - Wikipedia}},
url = {https://en.wikipedia.org/wiki/Alan{\_}Turing},
urldate = {2020-09-16},
year = {2020}
}
@misc{mnistdatabase,
author = {Wikipedia},
booktitle = {Wikipedia.org},
title = {{MNIST database}},
url = {https://en.wikipedia.org/wiki/MNIST{\_}database{\#}cite{\_}note-5},
urldate = {2020-10-27},
year = {2020}
}
@article{Kolda2007,
abstract = {This survey provides an overview of higher-order tensor decompositions, their applications, and available software. A tensor is a multidimensional or N -way array. Decompositions of higher-order tensors (i.e., N -way arrays with N ≥ 3) have applications in psychometrics, chemometrics, signal processing, numerical linear algebra, computer vision, numerical analysis, data mining, neuroscience, graph analysis, etc. Two particular tensor decompositions can be considered to be higher-order extensions of the matrix singular value decompo- sition: CANDECOMP/PARAFAC (CP) decomposes a tensor as a sum of rank-one tensors, and the Tucker decomposition is a higher-order form of principal components analysis. There are many other tensor decompositions, including INDSCAL, PARAFAC2, CANDELINC, DEDICOM, and PARATUCK2 as well as nonnegative variants of all of the above. The N-way Toolbox and Tensor Toolbox, both for MATLAB, and the Multilinear Engine are examples of software packages for working with tensors.},
author = {Kolda, Tamara G and Bader, Brett W},
file = {:Users/Tobias/Downloads/Kolda-Bader-SAND2007-6702.pdf:pdf},
journal = {SIAM Rev.},
keywords = {canonical decomposition (CANDECOMP),higher-order principal components analysis (Tucker,higher-order singular value decomposition (HOSVD),multilinear algebra,multiway arrays,parallel factors (PARAFAC),tensor decompositions},
number = {November},
pages = {455--500},
title = {{Tensor Decompositions and Applications}},
url = {http://www.ntis.gov/help/ordermethods.asp?loc=7-4-0{\#}online},
volume = {51},
year = {2009}
}
@article{Novikov2015,
abstract = {Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.},
archivePrefix = {arXiv},
arxivId = {1509.06569},
author = {Novikov, Alexander and Podoprikhin, Dmitry and Osokin, Anton and Vetrov, Dmitry},
eprint = {1509.06569},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/TensorizingNeuralNetworks.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {442--450},
title = {{Tensorizing neural networks}},
volume = {2015-Janua},
year = {2015}
}
@misc{MNIST,
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.},
title = {{The MNIST database of handwritten digits}},
year = {1998}
}
@article{Szegedy2015,
abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1109/CVPR.2015.7298594},
eprint = {1409.4842},
file = {:Users/Tobias/Downloads/07298594.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {1--9},
publisher = {IEEE},
title = {{Going deeper with convolutions}},
volume = {07-12-June},
year = {2015}
}
@article{numpy,
abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
archivePrefix = {arXiv},
arxivId = {2006.10256},
author = {Harris, Charles R. and Millman, K. Jarrod and van der Walt, St{\'{e}}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and van Kerkwijk, Marten H. and Brett, Matthew and Haldane, Allan and del R{\'{i}}o, Jaime Fern{\'{a}}ndez and Wiebe, Mark and Peterson, Pearu and G{\'{e}}rard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
doi = {10.1038/s41586-020-2649-2},
eprint = {2006.10256},
file = {:Users/Tobias/Downloads/s41586-020-2649-2.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7825},
pages = {357--362},
pmid = {32939066},
publisher = {Springer US},
title = {{Array programming with NumPy}},
url = {http://dx.doi.org/10.1038/s41586-020-2649-2},
volume = {585},
year = {2020}
}
@article{Yosinski2015,
abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pre-trained convnet with minimal setup.},
archivePrefix = {arXiv},
arxivId = {1506.06579},
author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
eprint = {1506.06579},
file = {:Users/Tobias/Downloads/Yosinski{\_}{\_}2015{\_}{\_}ICML{\_}DL{\_}{\_}Understanding{\_}Neural{\_}Networks{\_}Through{\_}Deep{\_}Visualization{\_}{\_}.pdf:pdf},
title = {{Understanding Neural Networks Through Deep Visualization}},
url = {http://arxiv.org/abs/1506.06579},
year = {2015}
}
@article{Kim2016,
abstract = {Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGG-S, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1 × 1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.},
archivePrefix = {arXiv},
arxivId = {1511.06530},
author = {Kim, Yong Deok and Park, Eunhyeok and Yoo, Sungjoo and Choi, Taelim and Yang, Lu and Shin, Dongjun},
eprint = {1511.06530},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Compression of deep convolutional neural networks for fast and low power mobile applications.pdf:pdf},
journal = {4th Int. Conf. Learn. Represent. ICLR 2016 - Conf. Track Proc.},
pages = {1--16},
title = {{Compression of deep convolutional neural networks for fast and low power mobile applications}},
year = {2016}
}
@article{Rigamonti2013,
abstract = {Learning filters to produce sparse image representations in terms of over complete dictionaries has emerged as a powerful way to create image features for many different purposes. Unfortunately, these filters are usually both numerous and non-separable, making their use computationally expensive. In this paper, we show that such filters can be computed as linear combinations of a smaller number of separable ones, thus greatly reducing the computational complexity at no cost in terms of performance. This makes filter learning approaches practical even for large images or 3D volumes, and we show that we significantly outperform state-of-the-art methods on the linear structure extraction task, in terms of both accuracy and speed. Moreover, our approach is general and can be used on generic filter banks to reduce the complexity of the convolutions. {\textcopyright} 2013 IEEE.},
author = {Rigamonti, Roberto and Sironi, Amos and Lepetit, Vincent and Fua, Pascal},
doi = {10.1109/CVPR.2013.355},
file = {:Users/Tobias/Downloads/Rigamonti{\_}Learning{\_}Separable{\_}Filters{\_}2013{\_}CVPR{\_}paper.pdf:pdf},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {2754--2761},
title = {{Learning separable filters}},
year = {2013}
}
@article{Denil2013,
abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95{\%} of the weights of a network without any drop in accuracy.},
archivePrefix = {arXiv},
arxivId = {1306.0543},
author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'aurelio and {De Freitas}, Nando},
eprint = {1306.0543},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Predicting parameters in deep learning.pdf:pdf},
issn = {10495258},
journal = {Adv. Neural Inf. Process. Syst.},
pages = {1--9},
title = {{Predicting parameters in deep learning}},
year = {2013}
}
@article{Kossaifi2016,
abstract = {Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed $\backslash$emph{\{}TensorLy{\}}, a high-level API for tensor methods and deep tensorized neural networks in Python. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and seamlessly integrates with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows users to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly},
archivePrefix = {arXiv},
arxivId = {1610.09555},
author = {Kossaifi, Jean and Panagakis, Yannis and Anandkumar, Anima and Pantic, Maja},
eprint = {1610.09555},
file = {:Users/Tobias/Library/Application Support/Mendeley Desktop/Downloaded/Kossaifi et al. - 2016 - TensorLy Tensor Learning in Python.pdf:pdf},
journal = {J. Mach. Learn. Res. 20},
pages = {1--6},
title = {{TensorLy: Tensor Learning in Python}},
url = {http://arxiv.org/abs/1610.09555},
volume = {20},
year = {2016}
}
@article{Oseledets2011,
abstract = {A simple nonrecursive form of the tensor decomposition in d dimensions is presented. It does not inherently suffer from the curse of dimensionality, it has asymptotically the same number of parameters as the canonical decomposition, but it is stable and its computation is based on low-rank approximation of auxiliary unfolding matrices. The new form gives a clear and convenient way to implement all basic operations efficiently. A fast rounding procedure is presented, as well as basic linear algebra operations. Examples showing the benefits of the decomposition are given, and the efficiency is demonstrated by the computation of the smallest eigenvalue of a 19-dimensional operator.},
author = {Oseledets, Ivan V},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/tensorTrain.pdf:pdf},
journal = {SIAM J. Sci. Comput.},
keywords = {060658576,10,1137,49j40,49j53,49k40,90c31,90c33,affine variational inequalities,ams subject classifications,cocoercivity,doi,dunn property,lipschitz con-,monotone operators,multifunctions,polyhedral multifunctions,psd-plus,tinuity},
pages = {2295--2317},
title = {{Tensor-Train Decomposition}},
volume = {33},
year = {2011}
}
@article{Mørup2011,
abstract = {Tensor (multiway array) factorization and decomposition has become an important tool for datamining. Fueled by the computational power of modern computer researchers can now analyze large-scale tensorial structured data that only a few years ago would have been impossible. Tensor factorizations have several advantages over two-way matrix factorizations including uniqueness of the optimal solution and component identification even when most of the data is missing. Furthermore, multiway decomposition techniques explicitly exploit the multiway structure that is lost when collapsing some of the modes of the tensor in order to analyze the data by regular matrix factorization approaches. Multiway decomposition is being applied to new fields every year and there is no doubt that the future will bring many exciting new applications. The aim of this overview is to introduce the basic concepts of tensor decompositions and demonstrate some of themany benefits and challenges of modeling data multiway for a wide variety of data and problem domains. {\textcopyright} 2011 John Wiley {\&} Sons, Inc.},
author = {M{\o}rup, Morten},
doi = {10.1002/widm.1},
file = {:Users/Tobias/Downloads/widm.1.pdf:pdf},
issn = {19424787},
journal = {Wiley Interdiscip. Rev. Data Min. Knowl. Discov.},
number = {1},
pages = {24--40},
title = {{Applications of tensor (multiway array) factorizations and decompositions in data mining}},
volume = {1},
year = {2011}
}
@article{Beavers2013,
abstract = {In this 2013 winner of the prestigious R.R. Hawkins Award from the Association of American Publishers, as well as the 2013 PROSE Awards for Mathematics and Best in Physical Sciences {\&} Mathematics, also from the AAP, readers will find many of the most significant contributions from the four-volume set of the Collected Works of A. M. Turing. These contributions, together with commentaries from current experts in a wide spectrum of fields and backgrounds, provide insight on the significance and contemporary impact of Alan Turing's work. Offering a more modern perspective than anything currently available, Alan Turing: His Work and Impact gives wide coverage of the many ways in which Turing's scientific endeavors have impacted current research and understanding of the world. His pivotal writings on subjects including computing, artificial intelligence, cryptography, morphogenesis, and more display continued relevance and insight into today's scientific and technological landscape. This collection provides a great service to researchers, but is also an approachable entry point for readers with limited training in the science, but an urge to learn more about the details of Turing's work.2013 winner of the prestigious R.R. Hawkins Award from the Association of American Publishers, as well as the 2013 PROSE Awards for Mathematics and Best in Physical Sciences {\&} Mathematics, also from the AAPNamed a 2013 Notable Computer Book in Computing Milieux by Computing ReviewsAffordable, key collection of the most significant papers by A.M. TuringCommentary explaining the significance of each seminal paper by preeminent leaders in the fieldAdditional resources available online},
author = {Beavers, Anthony F},
file = {:Users/Tobias/Downloads/10.1.1.225.255.pdf:pdf},
isbn = {978-0-12-386980-7},
journal = {Alan Turing His Work Impact},
pages = {481--485},
title = {{Alan Turing : Mathematical Mechanist}},
year = {2013}
}
@article{Lebedev2015,
abstract = {We propose a simple two-step approach for speeding up convolution layers within large convolutional neural networks based on tensor decomposition and discriminative fine-tuning. Given a layer, we use non-linear least squares to compute a low-rank CP-decomposition of the 4D convolution kernel tensor into a sum of a small number of rank-one tensors. At the second step, this decomposition is used to replace the original convolutional layer with a sequence of four convolutional layers with small kernels. After such replacement, the entire network is fine-tuned on the training data using standard backpropagation process. We evaluate this approach on two CNNs and show that it is competitive with previous approaches, leading to higher obtained CPU speedups at the cost of lower accuracy drops for the smaller of the two networks. Thus, for the 36-class character classification CNN, our approach obtains a 8.5x CPU speedup of the whole network with only minor accuracy drop (1{\%} from 91{\%} to 90{\%}). For the standard ImageNet architecture (AlexNet), the approach speeds up the second convolution layer by a factor of 4x at the cost of 1{\%} increase of the overall top-5 classification error.},
archivePrefix = {arXiv},
arxivId = {1412.6553},
author = {Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
eprint = {1412.6553},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Speeding-up CNNs using CP.pdf:pdf},
journal = {3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.},
pages = {1--11},
title = {{Speeding-up convolutional neural networks using fine-tuned CP-decomposition}},
year = {2015}
}
@article{pytorch,
archivePrefix = {arXiv},
arxivId = {arXiv:1912.01703v1},
author = {Paszke, Adam and Gross, Sam and Bradbury, James and Lin, Zeming and Devito, Zach and Massa, Francisco and Steiner, Benoit and Killeen, Trevor and Yang, Edward},
eprint = {arXiv:1912.01703v1},
file = {:Users/Tobias/Downloads/1912.01703.pdf:pdf},
number = {NeurIPS},
title = {{PyTorch : An Imperative Style , High-Performance Deep Learning Library}},
year = {2019}
}
@article{Jaderberg2014,
abstract = {The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5× speedup with no loss in accuracy, and 4.5× speedup with less than 1{\%} drop in accuracy, still achieving state-of-the-art on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {1405.3866},
author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.5244/c.28.88},
eprint = {1405.3866},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/jaderberg.pdf:pdf},
journal = {BMVC 2014 - Proc. Br. Mach. Vis. Conf. 2014},
title = {{Speeding up convolutional neural networks with low rank expansions}},
year = {2014}
}
@article{Gourgari2013,
abstract = {The detection and classification of human movements, as a joint field of Computer Vision and Pattern Recognition, is used with an increasing rate in applications designed to describe human activity. Such applications require efficient methods and tools for the automatic analysis and classification of motion capture data, which constitute an active field of research. To facilitate the development and the benchmarking of methods for action recognition, several video collections have previously been proposed. In this paper, we present a new video database that can be used for an objective comparison and evaluation of different motion analysis and classification methods. The database contains video clips that capture the 3D motion of individuals. To be more specific, the set consists of 8374 video clips, which contain 12 different types of tennis actions performed by 55 individuals, captured by Kinect. Kinect provides the depth map of motion data and helps to extract the 3D skeletal joint connections. Performing experiments using state of the art algorithms, the database shows to be very challenging. It contains very similar to each other actions, offering the opportunity to algorithms dedicated to gaming and athletics, to be developed and tested. The database is freely available for research purposes. {\textcopyright} 2013 IEEE.},
author = {Gourgari, Sofia and Goudelis, Georgios and Karpouzis, Konstantinos and Kollias, Stefanos},
doi = {10.1109/CVPRW.2013.102},
file = {:Users/Tobias/Downloads/thetis.pdf:pdf},
isbn = {9780769549903},
issn = {21607508},
journal = {IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. Work.},
keywords = {3D representation,Human action recognition,Kinect,dataset},
pages = {676--681},
title = {{THETIS: Three dimensional tennis shots a human action dataset}},
year = {2013}
}
@article{Liu2015,
abstract = {Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90{\%} of parameters, with a drop of accuracy that is less than 1{\%} on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.},
author = {Liu, Baoyuan and Wang, Min and Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
doi = {10.1109/CVPR.2015.7298681},
file = {:Users/Tobias/Google Drev/UNI/Master-Thesis-Fall-2020/Literature/Introduction/07298681.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.},
pages = {806--814},
publisher = {IEEE},
title = {{Sparse Convolutional Neural Networks}},
volume = {07-12-June},
year = {2015}
}
@article{Nakajima2013,
abstract = {The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, finding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF-the global solution can be analytically computed. More specifically, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a cuartic ecuation with its coefficients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis. {\textcopyright} 2013 Shinichi Nakajima, Masashi Sugiyama, S. Derin Babacan and Ryota Tomioka.},
author = {Nakajima, Shinichi and Sugiyama, Masashi and Babacan, S. Derin and Tomioka, Ryota},
file = {:Users/Tobias/Downloads/nakajima13a.pdf:pdf},
issn = {15324435},
journal = {J. Mach. Learn. Res.},
keywords = {Empirical Bayes,Matrix factorization,Model-induced regularization,Probabilistic PCA,Variational Bayes},
number = {1},
pages = {1--37},
title = {{Global analytic solution of fully-observed variational bayesian matrix factorization}},
volume = {14},
year = {2013}
}
